# 争议词汇修改落地文档

## 1. 实施说明
本文档是的争议词汇修改实施手册，包含所有已经修改的内容和明确的替换方案。本修改计划旨在将项目中的争议词汇替换为中性、合规的表述，同时确保项目在修改后能够正常运行。修改范围包括中文争议词汇和英文争议词汇，涵盖文件名、变量名、函数名、注释等所有代码元素。本次举措旨在更加符合本项目的仅供个人学习使用，仅用于公开信息浏览的宗旨和思想

## 2. 最终确定的词汇替换表

### 2.1 中文词汇替换（统一方案）
| 原词汇   | 替换为（统一）                   |
|----------|--------------------------------|
| 监控     | 公开内容查看                     |
| 爬取     | 公开信息浏览                     |
| 反爬     | 请求间隔优化                     |
| 反检测   | 访问策略适配                     |
| 目标     | 学习用公开平台                   |
| 抓取     | 公开信息整理                     |
| 爬虫     | 数据收集脚本                     |
| 机器人   | 智能处理程序                     |

### 2.2 英文词汇替换（统一方案）
| 原词汇   | 替换为（统一）                   |
|----------|--------------------------------|
| spider   | collector                       |
| scrape   | fetch                           |
| crawl    | browse                          |
| scraper  | fetcher                         |
| bot      | module                          |

## 3. 详细修改内容与实施步骤

### 3.1 文件名修改
**原文件**：`spider_v2.py`  
**新文件**：`collector.py`  
**修改后**：
1. 在文件系统中直接重命名文件
2. 更新所有引用该文件的地方：
   - `web_server.py` 第72行：`spider_v2.py` → `collector.py`
   - `web_server.py` 第112行：`spider_v2.py` → `collector.py`
   - `prompt_generator.py` 第36行：`spider_v2.py` → `collector.py`

### 3.2 核心函数名修改
**修改1：`src/scraper.py` 第55行**  
原函数名：`async def scrape_user_profile(context, user_id: str) -> dict:`  
新函数名：`async def fetch_user_profile(context, user_id: str) -> dict:`  
修改范围：函数定义 + 内部调用  
**修改后**：
- `src/scraper.py` 第392行：`scrape_user_profile` → `fetch_user_profile`

**修改2：`src/scraper.py` 第59行**  
原函数名：`async def scrape_xianyu(task_config: dict, debug_limit: int = 0):`  
新函数名：`async def fetch_xianyu(task_config: dict, debug_limit: int = 0):`  
修改范围：函数定义 + 外部调用  
**修改后**：
- `collector.py`（原spider_v2.py）第12行：`scrape_xianyu` → `fetch_xianyu`

### 3.3 核心变量名修改
**修改1：`web_server.py` 第16行**  
原变量名：`scraper_processes = {}`  
新变量名：`fetcher_processes = {}`  
修改范围：整个文件所有出现 `scraper_processes` 的位置

**修改2：`web_server.py` 第259行**  
原变量名：`sort_by: str = "crawl_time"`  
新变量名：`sort_by: str = "fetch_time"`  
修改范围：仅当前行

**修改3：`web_server.py` 第218行**  
原内容：`return item.get("爬取时间", "")`  
新内容：`return item.get("公开信息浏览时间", "")`  
修改范围：仅当前行

### 3.4 日志和注释修改
**修改1：`collector.py`（原spider_v2.py）第1行**  
原内容：`description="闲鱼商品监控脚本，支持多任务配置和实时AI分析。"`  
新内容：`description="闲鱼商品公开内容查看脚本，支持多任务配置和实时AI分析。"`

**修改2：`collector.py` 第35行**  
原内容：`print("\n--- 开始执行监控任务 ---")`  
新内容：`print("\n--- 开始执行公开内容查看任务 ---")`

**修改3：`src/scraper.py` 第59行注释**  
原内容：`"异步爬取闲鱼商品数据，并对每个新发现的商品进行实时的、独立的AI分析和通知。"`  
新内容：`"异步浏览闲鱼商品数据，并对每个新发现的商品进行实时的、独立的AI分析和通知。"`

**修改4：`src/scraper.py` 第78行注释**  
原内容：`"# 反检测启动参数"`  
新内容：`"# 访问策略适配启动参数"`

**修改5：`src/scraper.py` 第107行注释**  
原内容：`"# 增强反检测脚本"`  
新内容：`"# 增强访问策略适配脚本"`

**修改6：`src/scraper.py` 第143行注释**  
原内容：`"# 步骤 0 - 模拟真实用户：先访问首页（重要的反检测措施）"`  
新内容：`"# 步骤 0 - 模拟真实用户：先访问首页（重要的访问策略适配措施）"`

**修改7：`src/scraper.py` 第146行**  
原内容：`log_time("[反爬] 在首页停留，模拟浏览...", task_name=task_name)`  
新内容：`log_time("[请求间隔优化] 在首页停留，模拟浏览...", task_name=task_name)`

**修改8：`src/scraper.py` 第160行**  
原内容：`log_time(f"目标URL: {search_url}", task_name=task_name)`  
新内容：`log_time(f"学习用公开平台URL: {search_url}", task_name=task_name)`

**修改9：`src/scraper.py` 第164行**  
原内容：`log_time("[反爬] 模拟用户查看页面...", task_name=task_name)`  
新内容：`log_time("[请求间隔优化] 模拟用户查看页面...", task_name=task_name)`

**修改10：`src/scraper.py` 第198行**  
原内容：`print("检测到闲鱼反爬虫验证弹窗 (baxia-dialog)，无法继续操作。")`  
新内容：`print("检测到闲鱼访问验证弹窗 (baxia-dialog)，无法继续操作。")`

**修改11：`src/scraper.py` 第211行**  
原内容：`print("检测到闲鱼反爬虫验证弹窗 (J_MIDDLEWARE_FRAME_WIDGET)，无法继续操作。")`  
新内容：`print("检测到闲鱼访问验证弹窗 (J_MIDDLEWARE_FRAME_WIDGET)，无法继续操作。")`

**修改12：`src/scraper.py` 第321行**  
原内容：`print("检测到闲鱼反爬验证 (FAIL_SYS_USER_VALIDATE)")`  
新内容：`print("检测到闲鱼访问验证 (FAIL_SYS_USER_VALIDATE)")`

**修改13：`src/scraper.py` 第371行**  
原内容：`"爬取时间": datetime.now().isoformat(),`  
新内容：`"公开信息浏览时间": datetime.now().isoformat(),`

**修改14：`src/scraper.py` 第417行**  
原内容：`log_time("[反爬] 执行一次主要的随机延迟以模拟用户浏览间隔...", task_name=task_name)`  
新内容：`log_time("[请求间隔优化] 执行一次主要的随机延迟以模拟用户浏览间隔...", task_name=task_name)`

**修改15：`src/scraper.py` 第430行**  
原内容：`print(f"\n爬取过程中发生未知错误: {e}")`  
新内容：`print(f"\n公开信息浏览过程中发生未知错误: {e}")`

**修改16：`src/version.py` 第6行**  
原内容：`"商品卡如果没有抓取刀图片则选择默认logo",`  
新内容：`"商品卡如果没有整理到图片则选择默认logo",`

**修改17：`src/prompt_utils.py` 第1行**  
原内容：`"为闲鱼监控机器人的AI分析模块生成一份全新的【分析标准】文本。"`  
新内容：`"为闲鱼公开内容查看智能处理程序的AI分析模块生成一份全新的【分析标准】文本。"`

**修改18：`src/notifier/channels.py` 第48/63/77/108/125/139行**  
原内容：`"测试通知 - 闲鱼智能监控机器人"`  
新内容：`"测试通知 - 闲鱼公开内容查看智能处理程序"`

**修改19：`src/notifier/channels.py` 第90/95行**  
原内容：`"group": "闲鱼监控"`  
新内容：`"group": "闲鱼公开内容查看"`

**修改20：`web_server.py` 第58行**  
原内容：`print("Web服务器正在关闭，正在终止所有爬虫进程...")`  
新内容：`print("Web服务器正在关闭，正在终止所有数据收集脚本进程...")`

**修改21：`web_server.py` 第61行**  
原内容：`print("所有爬虫进程已终止。")`  
新内容：`print("所有数据收集脚本进程已终止。")`

**修改22：`web_server.py` 第67行**  
原内容：`FastAPI(title="咸鱼智能监控机器人")`  
新内容：`FastAPI(title="咸鱼公开内容查看智能处理程序")`

**修改23：`web_server.py` 第79行注释**  
原内容：`"由调度器调用的函数，用于启动单个爬虫任务。"`  
新内容：`"由调度器调用的函数，用于启动单个数据收集脚本任务。"`

**修改24：`web_server.py` 第83行**  
原内容：`print(f"定时任务触发: 正在为任务 '{task_name}' 启动爬虫...")`  
新内容：`print(f"定时任务触发: 正在为任务 '{task_name}' 启动数据收集脚本...")`

**修改25：`web_server.py` 第90行注释**  
原内容：`"使用与Web服务器相同的Python解释器来运行爬虫脚本"`  
新内容：`"使用与Web服务器相同的Python解释器来运行数据收集脚本"`

**修改26：`web_server.py` 第98行注释**  
原内容：`"获取爬虫日志文件的内容。支持从指定位置增量读取和任务名称筛选。"`  
新内容：`"获取数据收集脚本日志文件的内容。支持从指定位置增量读取和任务名称筛选。"`

**修改27：`web_server.py` 第192行**  
原内容：`"else: # 默认为抓取时间"`  
新内容：`"else: # 默认为公开信息浏览时间"`

**修改28：`prompt_generator.py` 第1行**  
原内容：`"生成闲鱼监控机器人的分析标准文件，并自动更新config.json。"`  
新内容：`"生成闲鱼公开内容查看智能处理程序的分析标准文件，并自动更新config.json。"`

### 3.5 配置和文档修改
**修改1：`web_server.py` 第211/223行**  
原内容：`log_file_path = os.path.join("logs", "scraper.log")`  
新内容：`log_file_path = os.path.join("logs", "fetcher.log")`
